\providecommand{\main}{..}
\documentclass[../COS3712_Notes.tex]{subfiles}

\begin{document}
  \setcounter{chapter}{5}
  \chapter{From Vertices to Fragments}
    \begin{sidenote}{Textbook}
      In the textbook, this section corresponds to Chapter 8: From Geometry to Pixels.
    \end{sidenote}

    \concept{Clipping} involves eliminating objects that lie outside the viewing volume,
    and thus cannot be visible in the image.
    \concept{Rasterization} produces fragments from the remaining objects.
    These fragments can contribute to the final image.
    \concept{Hidden-surface~removal} determines which fragments correspond to objects
    that are visible, namely, those that are in the view volume and are not blocked
    from view by other objects closer to the camera.

    \section{Basic Implementation Strategies}
      \begin{figure}
        \begin{center}
          \includegraphics[width=0.7\textwidth]{\main/images/chapter06/graphics_process.png}
        \end{center}
        \caption{High-level view of the graphics process}
      \end{figure}

      In computer graphics, we start with an application program, and end with an image.
      We can consider this process as a black box whose inputs are the vertices and states
      defined in the program -- geometric objects, attributes, camera specifications --
      and whose output is an array of coloured pixels in the framebuffer.

      Within the black box, we must perform many tasks, including transformations,
      clipping, shading, hidden-surface removal, and rasterization of the primitives
      that can appear on the display.
      These tasks can be organised in a variety of ways, but regardless of the strategy
      we adopt, we must always do two things:
      \begin{enumerate}[nosep]
        \item We must pass every geometric object through the system, and
        \item we must assign a colour to every pixel in the colour buffer that is displayed.
      \end{enumerate}

      Suppose we think of what goes into the black box in terms of a single program
      that carries out the entire process.
      Because this program must assign a value to every pixel and must process every
      geometric primitive (and light source),
      we expect this program to contain at least two loops that iterate over these basic
      variables.
      The variable we choose to control the outer loop determines the flow of the entire
      implementation process.
      There are two fundamental strategies: the \concept{image-oriented} approach, and the
      \concept{object-oriented} approach.

      \begin{definition}{Object Oriented Approach}
        The outer loop iterates over the objects.

        A pipeline renderer fits this description.
        Vertices are defined by the program and flow through a sequence of modules that
        transforms them, colours them, and determines whether they are visible.

        After a polygon passes through geometric processing, the rasterization
        of this polygon can potentially affect any pixels in the framebuffer.

        Most implementations that follow this approach are based on construction of a
        rendering pipeline containing hardware or software modules for each of the tasks.
        Data (vertices) flow \emph{forward} through the system.

        In the past, the major limitations of the object-oriented approach were the
        large amount of memory required and the high cost of processing each object
        independently.
        Any geometric primitive that emerges from the geometric processing can potentially
        affect any set of pixels in the framebuffer.
        So, the entire colour buffer, and various other buffers, must be of the size of the
        display, and must be available at all times.

        Today, the main limitation of object-oriented implementations is that they cannot
        handle most global calculations.
        Because each geometric primitive is processed independently
        -- and in an arbitrary order --
        complex shading effects that involve multiple geometric objects,
        such as reflections,
        cannot be handled except by approximate methods.
        The major exception is hidden-surface removal, where the $z$-buffer is used to store
        global information.
      \end{definition}

      \begin{definition}{Image-Oriented Approach}
        The outer loop iterates over pixels, or rows of pixels called \concept{scan~lines},
        that constitute the framebuffer.

        For each pixel, we work \emph{backward}, trying to determine which geometric primitives
        can contribute to its colour.

        The advantages of this approach are that we need only limited display memory
        at any time and that we can hope to generate pixels
        at the rate and in the order required to refresh the display.
        Because the results of most calculations do not differ from pixel to pixel
        (or scan line to scan line),
        we can use this coherence in our algorithms by developing incremental forms
        for many of the steps in the implementation.

        The main disadvantage of this approach is that, unless we first build a data structure
        from the geometric data, we do not know which primitives affect which pixels.
        Such a data structure can be complex, and may imply that all the geometric data
        must be available at all times during the rendering process.
        For problems with very large databases, even having a good data representation
        may not avoid memory leaks.
        However, because image-space algorithms have access to all objects for each pixel,
        they are well suited to handle global effects, such as shadows and reflections.
      \end{definition}

      Within these two major categories specified by two loops, each may contain other loops.

    \section{Four Major Tasks}
      There are four major tasks that any graphics system must perform to render a geometric
      entity, as that entity passes from definition in a user program to possible display
      on an output device:
      \begin{enumerate}[nosep]
        \item Modelling
        \item Geometry processing
        \item Rasterization
        \item Fragment processing
      \end{enumerate}

      \begin{definition}{Modelling}
        The usual results of the modelling process are sets of vertices that specify
        a group of geometric objects supported by the rest of the system.
        We can look at the modeller as a black box that produces geometric objects
        and is usually a user program.

        There are other tasks a modeller might perform, such as clipping.
        A user can generate geometric objects in their program,
        and hope that the rest of the system can process these objects at the rate at which
        they are produced, or the modeller can attempt to ease the burden on the rest of the
        system by minimising the number of objects that it passes on.
        The latter approach often means that the modeller may do some of the same jobs
        as the result of the system, albeit with different algorithms.
        In the case of clipping, the modeller knows more about the specifics of the application,
        and can often use a good heuristic to eliminate many, if not most,
        primitives before they are sent on through the standard viewing process.
      \end{definition}

      \begin{definition}{Geometry Processing}
        Geometry processing works with vertices.
        The goals of the geometry processor are to determine which geometric objects
        appear on the display, and to assign shades or colours to the vertices of these objects.
        Four processes are required:
        \begin{enumerate}[nosep]
          \item Projection
          \item Primitive Assembly
          \item Clipping
          \item Shading
        \end{enumerate}

        Usually, the first step in geometry processing is to change representations
        from object coordinates to camera coordinates using the model-view transformation.
        The second step is to transform vertices using the projection transformation
        to a normalised view volume in which objects that might be visible are contained
        in a cube centred at the origin.
        Vertices are now represented in clip coordinates.

        Before clipping can take place, vertices must be grouped into objects,
        a process known as \concept{primitive assembly}.
        When vertices reach the rasterizer, they can no longer
        be processed individually, but only as primitives.

        After clipping takes place, the remaining vertices are still in four-dimensional
        homogeneous coordinates.
        Perspective division converts theme to 3D representations in normalised device coordinates.
        Per-vertex shading is also performed during this stage.

        Collectively, these operations constitute \concept{front-end~processing}.
        All are carried out on a vertex-by-vertex basis.
      \end{definition}

      \begin{definition}{Rasterization (Scan Conversion)}
        We need to retain depth information for hidden-surface removal.
        But only the $x, y$ values of the vertices are needed to determine which pixels
        in the framebuffer can be affected by the primitive.
        To generate a set of fragments that give the locations of the pixels in the framebuffer
        corresponding to these vertices, we only need their $x, y$ components,
        or, equivalently, the results of the orthogonal projection of these vertices.
        We determine these fragments through a process called \concept{rasterization}
        or \concept{scan~conversion}.
        For line segments, rasterization determines which fragments should be used to approximate
        a line segment between the projected vertices.
        For polygons, rasterization determines which pixels lie inside the 2D polygon
        determined by the projected vertices.

        The colours we assign to these fragments can be determined by the vertex attributes,
        or obtained by interpolating the shades at the vertices that are computed.
        Objects more complex than line segments and polygons are usually approximated
        by multiple line segments and triangles.

        The rasterizer starts with vertices in normalised device coordinates,
        but outputs fragments whose location are in units of the display
        -- \concept{window coordinates}.
        The projection of the clipping volume must appear in the assigned viewport.
        We use the term \concept{screen~coordinates} to refer to the 2D system that is the same
        as the window coordinates, but lacks the depth coordinate.
      \end{definition}

      \begin{definition}{Fragment Processing}
        In the simplest situations, each fragment is assigned a colour by the rasterizer,
        and this colour is placed in the framebuffer at the locations corresponding to the
        fragment's location.

        Per-fragment shading, texture-mapping, bump-mapping, alpha blending,
        antialiasing, and hidden-surface removal all take place in this stage.
      \end{definition}

\end{document}
