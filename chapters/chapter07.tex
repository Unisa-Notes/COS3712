\providecommand{\main}{..}
\documentclass[../COS3712_Notes.tex]{subfiles}

\begin{document}
  \setcounter{chapter}{6}
  \chapter{Discrete Techniques}
    \section{Buffers}
      What all buffers have in common is that they are inherently discrete:
      they have limited resolution, both spatially and in depth.
      We can define a (two-dimensional) \concept{buffer} as block of memory with
      $n \times m$ $k$-bit elements.

      The \concept{framebuffer} is the set of buffers that the graphics system uses for rendering,
      including the front and back colour buffers, the depth buffer, and other buffers
      the hardware may provide.

    \section{Mapping Methods}
      One of the most powerful uses of discrete data is for surface rendering.
      The process of modelling an object by a set of geometric primitives and then rendering
      these primitives has its limitations.

      An alternative is not to attempt to build increasingly more complex models,
      but rather to build a simple model and to add detail as part of the rendering process.
      As the implementation renders a surface, it generates a set of fragments,
      each of which corresponds to a pixel in the framebuffer.
      Fragments carry colour, depth, and other information that can be used to determine
      how they contribute to the pixels in which they correspond.
      As part of the rasterization process, we must assign a shade or colour to each fragment.
      These colours can be modified during fragment processing after rasterization.
      The mapping algorithms can be thought of as wither modifying the shading algorithm
      based on a 2D-array, the map, or modifying the shading by using the map to alter the
      surface using three major techniques:
      \begin{itemize}[nosep]
        \item Texture mapping
        \item Bump mapping
        \item Environment mapping
      \end{itemize}

      \begin{definition}{Texture Mapping}
        Uses an image (or texture) to influence the colour of a fragment.
        Textures can be specified using a fixed pattern; by a procedural texture generation
        method; or through a digitized image.
        In all cases, we can characterise the resulting image as the mapping of a texture
        to a surface, which is carried out as part of the rendering of the surface.
      \end{definition}

      \begin{definition}{Bump Maps}
        Unlike texture maps, which paint patterns onto smooth surfaces,
        \concept{bump~maps} distort the normal vectors during the shading process to make the
        surface appear to have small variations in shape.
      \end{definition}

      \begin{definition}{Environment Maps (Reflection Maps)}
        Allow us to create images that have the appearance of reflected materials
        without having to trace reflected rays.
        An image of the environment is painted onto the surface as that surface is being
        rendered.
      \end{definition}

      \begin{sidenote}{Similarities Between the Three Methods}
        $ $\vspace{-1em}
        \begin{itemize}[nosep]
          \item All three alter the shading of individual fragments as part of fragment processing.
          \item All rely on the map being stored as a one-, two-, or three-dimensional image.
          \item All keep the geometric complexity low while creating the illusion of complex
            geometry.
          \item All are subject to aliasing errors.
        \end{itemize}
      \end{sidenote}

      Two-dimensional texture mapping is supported by WebGL.
      Environment maps are a special case of standard texture mapping, but can be altered
      to create a variety of new effects in the fragment shader.
      Bump mapping requires us to process each fragment independently,
      something we can do with a fragment shader.

    \section{Two-Dimensional Texture Mapping}
      Textures are patterns, and can be one-, two-, three, or four-dimensional.
      Because the use of surfaces is so important in computer graphics,
      mapping 2D textures to surfaces is by far the most common use of texture mapping.

      Although there are multiple approaches to texture mapping, all require a sequence
      of steps that involve mappings around three or four different coordinate systems:
      \begin{descriptimize}[nosep]
        \item[Screen coordinates.] Where the final image is produced.
        \item[Object coordinates.] Where we describe the objects on which the textures will
          be mapped.
        \item[Texture coordinates.] Used to locate positions in the texture.
        \item[Parametric coordinates.] Used to specify parametric surfaces.
      \end{descriptimize}

      In most applications, textures start out as 2D images.
      These images are brought into processor memory as arrays.
      We call the elements of these arrays \concept{texels (texture elements)},
      rather than pixels, to emphasise how they will be used.
      We prefer to think of this array as a continuous rectangular two-dimensional texture pattern
      $T(s, t)$.
      The independent variables $s$ and $t$ are known as \concept{texture coordinates}.
      We can scale our texture coordinates to vary over the interval $[0.0, 1.0]$.

      A \concept{texture map} associates a texel with each point on a geometric object
      that is itself mapped to screen coordinates for display.

      One way to think about texture mapping is in terms of two concurrent mappings:
      the first from texture coordinates to object coordinates,
      and the second from parametric coordinates to object coordinates.
      A third mapping takes us from object coordinates to screen coordinates.

      Conceptually, the texture mapping process is simple.
      A small area of texture pattern maps to the area of the geometric surface,
      corresponding to a pixel in the final image.

\end{document}
